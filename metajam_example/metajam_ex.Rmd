---
title: "Metajam Example"
output: html_document
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(
  list(
    echo = TRUE,
    collapse = TRUE)
  )
```

Suppose we have have a link to a dataset in a data repository and we want to import it into our R session. For this example, we'll use the Stream water chemistry data for Green Lake 1, 1985-2010: https://portal.edirepository.org/nis/mapbrowse?packageid=knb-lter-nwt.107.10

The `metajam` package gives us a convenient way to get the data (and the metadata) into our `R` session.

#### Step 1: Install/load the metajam package
```{r, message= FALSE}
#devtools::install_github("NCEAS/metajam")
library(metajam)
library(tidyverse) # for convenience
library(here) # for file path management
```

#### Step 2: Find the link to the dataset
Go to the web address for the dataset and find the download button for the data. In general, right-clicking this and clicking "Copy Link" should do the trick. 

```{r, out.width= "50%", echo= FALSE}
include_graphics(path = here("metajam_example", "metajam_dataset_link.png"))
```

In our case this link is: https://portal.edirepository.org/nis/dataviewer?packageid=knb-lter-nwt.107.10&entityid=aef605d3ab0fba82ec95f665ff4b066b

#### Step 3: Choose where you want the files to be saved
In our case, we'll just put it into the metajam_example folder.
```{r}
#eg desired_path_to_data <- "~/Desktop"
desired_path_to_data <- here("metajam_example")
```


#### Step 4: Download the data by pasting the link you just copied
```{r}
# this will download the data into a folder and save the path to that folder
downloaded_data <- download_d1_data("https://portal.edirepository.org/nis/dataviewer?packageid=knb-lter-nwt.107.10&entityid=aef605d3ab0fba82ec95f665ff4b066b", path = desired_path_to_data)
```

#### Step 5: Now read in the data (with all the metadata)
```{r, message = FALSE}
my_data <- read_d1_files(downloaded_data)
```


Let's take a look at what we just read into `R`:
```{r}
summary(my_data)
```

It's a list of 4 dataframes! With this, everything we need is inside our `R` environment. 

#### Taking a deeper look at each of these dataframes

The dataset of interest:
```{r}
my_data$data
```

It's a good idea at this point to check that the column types are all correct. For instance, we notice that the `ANC` column is a character vector, rather than numeric. If we look a little deeper at this column, we find that this is because there are a few entries that say "NP" instead of a number.
```{r}
my_data$data %>%
  filter(ANC == "NP")
```


__Question__: What does NP mean in the `ANC` column?

To answer this, we can look at the attribute metadata that came with this dataset, which looks like this:
```{r}
my_data$attribute_metadata
```

To answer our question, we can just pull out the part of the dataframe we're interested in:
```{r}
my_data$attribute_metadata %>%
  filter(attributeName == "ANC") %>%
  select(attributeName, attributeDefinition)
```
So this tells us that NP means "not performed"! We can now deal with this column however is most appropriate for our analysis (for example, we can convert the NP into missing values and make the column numeric)

__Question__: What are the exact coordinate boundaries of the dataset?  
For this, we can look at the summary metadata:
```{r}
my_data$summary_metadata
```

To answer our question, we just subset the data we're interested in:
```{r}
# pull out all of the rows where name contains "BoundingCoordinate"
my_data$summary_metadata %>%
  filter(str_detect(name, "BoundingCoordinate"))
```

These are just a few examples of the kinds of questions we can answer with the metadata! Hopefully this gives you enough to speed up the data importing stage and get started with your analysis.

-------

_Note_: There is also a `factor_metadata` dataframe inside `my_data`. This dataset doesn't contain any factor variables, so `factor_metadata` is an empty dataframe that can be ignored. 

For more information, see the documentation and articles here: https://nceas.github.io/metajam/





--------

## Formatting data
Following the format of `stream_data_template.xlsx`, let's first import the template to see what columns we need
```{r}
# Read in the template
template <- readxl::read_excel(here("metajam_example", "Stream_Data_Template.xlsx"), 
                               sheet = "Raw Data"
                               )

# The new data
greenlake_data <- my_data$data
```


Next, let's create a new dataframe with the same number of rows as our new data

```{r}
# Create empty dataframe shell with new data's number of rows
formatted_df <- tibble(.rows = nrow(greenlake_data)) 
```

Now we need to match the column names between the two tables. To do this, we'll try to make a lookup-table with the `template` column being the desired column names, and `greenlake` being the names from the new data we just downloaded.  

We'll start this lookup table by using the `agrep()` function to fuzzy-match the two sets of names. The code below looks at each template name, and tries to find a match using the column names of `greenlake_data`. If it cannot find a close match, it will return an empty string.
```{r}
# Start by matching by closest name as a first pass
(fuzzy_match <- tibble(template = names(template)) %>%
  rowwise() %>%
  mutate(greenlake = max("", agrep(template, names(greenlake_data), 
                                   value = T))) %>% # max is a little hacky..
  ungroup()
)
```


This was an OK first pass, but we can see that there's still a lot of blanks (eg Site/Stream Name) didn't find a match, but there's also some missed matches (eg `Na` and `K` were both matched up with `year`). 

To see if we can find a true match, we can look into the name descriptions using the below code.
```{r}
# To see if we can match up the other columns, check the documentation
my_data$attribute_metadata %>% 
  select(attributeName, attributeDefinition) 

```


Now that we have a better idea of which columns match up, we can fill in the missing / incorrect entries in our table. For example, the code below matches "Site/Stream Name" to "local_site" , matches "Sampling Date" with "date", and turns the incorrect matches "Na" and "K" to both be "".
__Note:__ The last line `TRUE ~ greenlake` tells `R` to keep the values from the `fuzzy_match` unless you overwrite them here. E.g without that last line, "LTER" and "Time" (two of the correctly matched columns) would be overwritten as `NA`.
```{r}
# Fill in the columns that didn't match, and correct the wrongly corrected matches
# continue in the same way until you've filled out everything you could.
(lookup_table <- fuzzy_match %>%
  mutate(greenlake = case_when(
    template == "Site/Stream Name" ~ "local_site",
    template == "Sampling Date" ~ "date",
    template == "alkalinity" ~ "alkal",
    template == "Na" ~ "",
    template == "K" ~ "",
    TRUE ~ greenlake
    )
    )
)
```

Once the lookup table is as filled out as possible, we can remove all rows that still had no match (which we represented by an empty string in the `greenlake` column:
```{r}
lookup_table <- lookup_table %>%
  filter(greenlake != "")
```


```{r}
# rename operation
names(greenlake_data) = lookup_table$greenlake[match(names(template), lookup_table$template)]


# bind_rows, now that the column names match.
formatted_df <- formatted_df %>%
  bind_rows(greenlake_data)

```






